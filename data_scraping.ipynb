{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "326355d0",
   "metadata": {},
   "source": [
    "# URLs of Pages, Books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396791b",
   "metadata": {},
   "source": [
    "The webpage has more than 900 pages, and each page has a certain amount of books represented on it. Thus the script below is gathering page urls and urls of all books represented on all the pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deed6cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL\n",
    "base_url = \"https://zangakbookstore.am/en/grqer?page=\"\n",
    "\n",
    "# Create a list to store the page URLs\n",
    "page_urls = []\n",
    "\n",
    "# Iterate through page numbers from 1 to 952\n",
    "for page_number in range(1, 953):\n",
    "    page_url = base_url + str(page_number)\n",
    "    page_urls.append(page_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL\n",
    "books_urls = []\n",
    "\n",
    "# Iterate through page URLs\n",
    "for url in page_urls:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all the 'a' tags with the specified structure\n",
    "        a_tags = soup.find_all(\"a\", class_=\"d-inline-block position-relative\")\n",
    "\n",
    "        # Extract the 'href' attribute from each 'a' tag\n",
    "        for a in a_tags:\n",
    "            href = a.get(\"href\")\n",
    "            if href:\n",
    "                #print(href)\n",
    "                books_urls.append(href)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the web page: {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d94f81c",
   "metadata": {},
   "source": [
    "# Scraping Book Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73251645",
   "metadata": {},
   "source": [
    "The script below scrapes the information about each book by visiting it's URL link and scraping the data from the certain page. Later on, a data frame is created including information about all books, and is saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c432f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file to store the data\n",
    "with open(\"Books.csv\", \"w\", newline=\"\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow([\"Title\", \"Author\", \"Price\", \"Publisher\", \"ISBN\", \"Publishing Year\", \"Language\", \"Age License\", \"Cover Type\", \"Pages Number\"])\n",
    "\n",
    "    # Iterate through book URLs\n",
    "    for url in books_urls:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                title = soup.find(\"h1\", class_=\"product-name mb-4 d-none d-md-block\").text.strip() if soup.find(\"h1\", class_=\"product-name mb-4 d-none d-md-block\") else \"N/A\"\n",
    "                author = soup.find(\"a\", class_=\"color-main text-decoration-none author-btn\").text.strip() if soup.find(\"a\", class_=\"color-main text-decoration-none author-btn\") else \"N/A\"\n",
    "                price = soup.find(\"div\", class_=\"product-price-view mb-2\").text.strip() if soup.find(\"div\", class_=\"product-price-view mb-2\") else \"N/A\"\n",
    "\n",
    "               # Extract data from the \"tab_details\" section with error handling\n",
    "                details_section = soup.find(\"div\", id=\"tab_details\")\n",
    "                publisher = details_section.find(\"div\", string=\"Publishing house\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Publishing house\") else \"N/A\"\n",
    "                isbn = details_section.find(\"div\", string=\"EAN\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"EAN\") else \"N/A\"\n",
    "                publishing_year = details_section.find(\"div\", string=\"Year\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Year\") else \"N/A\"\n",
    "                language = details_section.find(\"div\", string=\"Language\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Language\") else \"N/A\"\n",
    "                age_license = details_section.find(\"div\", string=\"Age\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Age\") else \"N/A\"\n",
    "                cover_type = details_section.find(\"div\", string=\"Coating\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Coating\") else \"N/A\"\n",
    "                pages_number = details_section.find(\"div\", string=\"Pages\").find_next(\"div\").text.strip() if details_section.find(\"div\", string=\"Pages\") else \"N/A\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping data from {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            # Write the data to the CSV file\n",
    "            writer.writerow([title, author, price, publisher, isbn, publishing_year, language, age_license, cover_type, pages_number])\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "print(\"Data has been scraped and saved to 'Books.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b3acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out the gathered DF\n",
    "pd.read_csv(\"Books.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c544a512",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7643739",
   "metadata": {},
   "source": [
    "Removing NAs, duplicates and inconvenient formatting of data will make things easier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2448f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "df = pd.read_csv(\"Books.csv\")\n",
    "df = df.dropna(axis=0).reset_index(drop=True)\n",
    "df.ISBN = df.ISBN.astype(int)\n",
    "df.to_csv('Books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 (in case of type errors)\n",
    "# df = pd.read_csv('Books-13000-14000pp.csv')\n",
    "# df['ISBN'] = df['ISBN'].str.replace(r'\\D', '', regex=True)\n",
    "# df = df.dropna(axis=0).reset_index(drop=True)\n",
    "# df.ISBN = df.ISBN.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
